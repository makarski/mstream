[[services]]
provider = "pubsub"
name = "pubsub-example"
auth = { kind = "service_account", account_key_path = "/path/to/service-account.json" }

[[services]]
provider = "pubsub"
name = "pubsub-static-token"
auth = { kind = "static_token", env_token_name = "PUBSUB_TOKEN" }

[[services]]
provider = "kafka"
name = "kafka-local"
"bootstrap.servers" = "localhost:9092"

[[services]]
provider = "kafka"
name = "kafka-cloud"
offset_seek_back_seconds = 3600 # Optional: Seek back 1h in time for Kafka source
"bootstrap.servers" = "your-kafka-broker:9092"
"security.protocol" = "SASL_SSL"
"sasl.mechanism" = "PLAIN"
"sasl.username" = "env:MSTREAM_KAFKA_USERNAME"
"sasl.password" = "env:MSTREAM_KAFKA_PASSWORD"
"client.id" = "mstream-client"
"group.id" = "mstream-group"

[[services]]
provider = "mongodb"
name = "mongodb-source"
connection_string = "mongodb://localhost:27017"
db_name = "example_db"
schema_collection = "test_schemas" # Optional: collection to store Avro schemas

[[services]]
provider = "http"
name = "http-local"
host = "http://localhost:8000"
max_retries = 5 # Optional: default is 5
base_backoff_ms = 1000 # Optional: default is 1000ms
connection_timeout_sec = 30
timeout_sec = 30 # Optional: default is 30 seconds
tcp_keepalive_sec = 300 # Optional: default is 300 seconds

[[services]]
provider = "http"
name = "transform-service"
host = "http://transform-api:8080"
max_retries = 3
connection_timeout_sec = 15
timeout_sec = 20

# Connector Configurations
[[connectors]]
name = "mongodb-source-connector"
# Configure the source MongoDB collection to watch for changes
source = { service_name = "mongodb-source", id = "example_collection", encoding = "bson" }
# Schema is optional - if omitted, no field filtering will be applied
# schema = { service_name = "pubsub-example", id = "projects/your-project/schemas/example-schema", encoding = "avro" }
sinks = [
    { service_name = "kafka-local", id = "local_topic", encoding = "json" },
    { service_name = "kafka-cloud", id = "cloud_topic", encoding = "avro" },
    { service_name = "pubsub-example", id = "projects/your-project/topics/example-topic", encoding = "avro" },
    { service_name = "mongodb-source", id = "example_collection_copy", encoding = "bson" },
    { service_name = "http-local", id = "http_sink", encoding = "json" }
]

[[connectors]]
name = "basic-mongodb-connector"
# Simple configuration without schema filtering
source = { service_name = "mongodb-source", id = "simple_collection", encoding = "bson" }
# No schema defined - all fields will be passed through
sinks = [
    { service_name = "kafka-local", id = "unfiltered_topic", encoding = "json" }
]

[[connectors]]
name = "kafka-source-connector"
# Configure Kafka as a source
source = { service_name = "kafka-cloud", id = "source_topic", encoding = "avro" }
# Schema is optional for filtering fields
schema = { service_name = "pubsub-example", id = "projects/your-project/schemas/kafka-source-schema", encoding = "avro" }
sinks = [
    { service_name = "kafka-local", id = "processed_topic", encoding = "json" },
    { service_name = "pubsub-example", id = "projects/your-project/topics/processed-topic", encoding = "avro" },
]

[[connectors]]
name = "pubsub-source-connector"
# Configure PubSub as a source
source = { service_name = "pubsub-example", id = "projects/your-project/subscriptions/example-subscription", encoding = "avro" }
# Schema configuration is optional
# schema = { service_name = "mongodb-source", id = "example-id", encoding = "avro" }
sinks = [
    { service_name = "kafka-cloud", id = "pubsub_data_topic", encoding = "avro" },
    { service_name = "kafka-local", id = "pubsub_data_local", encoding = "json" },
]

[[connectors]]
name = "middleware-example-connector"
source = { service_name = "mongodb-source", id = "users", encoding = "bson" }
# Define middleware chain for transformations
middlewares = [
    # First middleware transforms the data (e.g., normalize fields)
    # The 'id' field now specifies just the path without the host
    { service_name = "transform-service", id = "normalize", encoding = "json" },
    # Second middleware enriches the data
    { service_name = "http-local", id = "enrich", encoding = "json" },
    # Third middleware filters or validates the data
    { service_name = "transform-service", id = "validate", encoding = "json" },
]
sinks = [
    { service_name = "kafka-local", id = "transformed_users", encoding = "json" },
    { service_name = "mongodb-source", id = "processed_users", encoding = "bson" },
]

[[connectors]]
name = "avro-middleware-connector"
source = { service_name = "kafka-cloud", id = "source_events", encoding = "avro" }
# Middleware with different encoding transformations
middlewares = [
    # The 'id' field is now just the path that will be appended to the host URL
    { service_name = "transform-service", id = "process", encoding = "json" },
    # Result is sent as JSON to the next middleware
    { service_name = "http-local", id = "analyze", encoding = "json" },
]
sinks = [
    { service_name = "pubsub-example", id = "projects/your-project/topics/processed-events", encoding = "avro" },
]
