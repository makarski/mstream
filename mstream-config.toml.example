[[services]]
provider = "pubsub"
name = "pubsub-example"
auth = { kind = "service_account", account_key_path = "/path/to/service-account.json" }

[[services]]
provider = "pubsub"
name = "pubsub-static-token"
auth = { kind = "static_token", env_token_name = "PUBSUB_TOKEN" }

[[services]]
provider = "kafka"
name = "kafka-local"
"bootstrap.servers" = "localhost:9092"

[[services]]
provider = "kafka"
name = "kafka-cloud"
offset_seek_back_seconds = 3600 # Optional: Seek back 1h in time for Kafka source
"bootstrap.servers" = "your-kafka-broker:9092"
"security.protocol" = "SASL_SSL"
"sasl.mechanism" = "PLAIN"
"sasl.username" = "env:MSTREAM_KAFKA_USERNAME"
"sasl.password" = "env:MSTREAM_KAFKA_PASSWORD"
"client.id" = "mstream-client"
"group.id" = "mstream-group"

[[services]]
provider = "mongodb"
name = "mongodb-source"
connection_string = "mongodb://localhost:27017"
db_name = "example_db"

[[services]]
provider = "http"
name = "http-local"
host = "http://localhost:8000"
max_retries = 5 # Optional: default is 5
base_backoff_ms = 1000 # Optional: default is 1000ms
connection_timeout_sec = 30
timeout_sec = 30 # Optional: default is 30 seconds
tcp_keepalive_sec = 300 # Optional: default is 300 seconds

[[services]]
provider = "http"
name = "transform-service"
host = "http://transform-api:8080"
max_retries = 3
connection_timeout_sec = 15
timeout_sec = 20

# Connector Configurations
[[connectors]]
name = "mongodb-source-connector"
# Configure the source MongoDB collection to watch for changes
# Optional: reference a schema by ID
source = { service_name = "mongodb-source", resource = "example_collection", output_encoding = "bson", schema_id = "basic_schema" }
# Define available schemas
schemas = [
    { id = "basic_schema", service_name = "pubsub-example", resource = "projects/your-project/schemas/example-schema" },
    { id = "extended_schema", service_name = "pubsub-example", resource = "projects/your-project/schemas/extended-schema" }
]
sinks = [
    { service_name = "kafka-local", resource = "local_topic", output_encoding = "json", schema_id = "basic_schema" },
    { service_name = "kafka-cloud", resource = "cloud_topic", output_encoding = "avro", schema_id = "extended_schema" },
    { service_name = "pubsub-example", resource = "projects/your-project/topics/example-topic", output_encoding = "avro", schema_id = "basic_schema" },
    { service_name = "mongodb-source", resource = "example_collection_copy", output_encoding = "bson" },
    { service_name = "http-local", resource = "http_sink", output_encoding = "json" }
]

[[connectors]]
name = "basic-mongodb-connector"
# Simple configuration without schema filtering
source = { service_name = "mongodb-source", resource = "simple_collection", output_encoding = "bson" }
# No schemas defined - all fields will be passed through
sinks = [
    { service_name = "kafka-local", resource = "unfiltered_topic", output_encoding = "json" }
]

[[connectors]]
name = "kafka-source-connector"
# Configure Kafka as a source
source = { service_name = "kafka-cloud", resource = "source_topic", input_encoding = "avro", output_encoding = "json", schema_id = "kafka_schema" }
# Schema is optional for filtering fields
schemas = [
    { id = "kafka_schema", service_name = "pubsub-example", resource = "projects/your-project/schemas/kafka-source-schema" }
]
sinks = [
    { service_name = "kafka-local", resource = "processed_topic", output_encoding = "json" },
    { service_name = "pubsub-example", resource = "projects/your-project/topics/processed-topic", output_encoding = "avro", schema_id = "kafka_schema" },
]

[[connectors]]
name = "pubsub-source-connector"
# Configure PubSub as a source
source = { service_name = "pubsub-example", resource = "projects/your-project/subscriptions/example-subscription", input_encoding = "avro", output_encoding = "json" }
# Schema configuration is optional
schemas = [
    { id = "pubsub_schema", service_name = "mongodb-source", resource = "example-schema-collection" }
]
sinks = [
    { service_name = "kafka-cloud", resource = "pubsub_data_topic", output_encoding = "avro", schema_id = "pubsub_schema" },
    { service_name = "kafka-local", resource = "pubsub_data_local", output_encoding = "json" },
]

[[connectors]]
name = "middleware-example-connector"
source = { service_name = "mongodb-source", resource = "users", output_encoding = "bson", schema_id = "user_schema" }
# Define available schemas
schemas = [
    { id = "user_schema", service_name = "pubsub-example", resource = "projects/your-project/schemas/user-schema" },
    { id = "enriched_schema", service_name = "pubsub-example", resource = "projects/your-project/schemas/enriched-user" }
]
# Define middleware chain for transformations
middlewares = [
    # First middleware transforms the data (e.g., normalize fields)
    { service_name = "transform-service", resource = "normalize", schema_id = "user_schema", output_encoding = "json" },
    # Second middleware enriches the data
    { service_name = "http-local", resource = "enrich", schema_id = "enriched_schema", output_encoding = "json" },
    # Third middleware filters or validates the data
    { service_name = "transform-service", resource = "validate", schema_id = "enriched_schema", output_encoding = "json" },
]
sinks = [
    { service_name = "kafka-local", resource = "transformed_users", output_encoding = "json", schema_id = "enriched_schema" },
    { service_name = "mongodb-source", resource = "processed_users", output_encoding = "bson", schema_id = "user_schema" },
]

[[connectors]]
name = "avro-middleware-connector"
source = { service_name = "kafka-cloud", resource = "source_events", input_encoding = "avro", output_encoding = "avro", schema_id = "events_schema" }
# Define schemas
schemas = [
    { id = "events_schema", service_name = "pubsub-example", resource = "projects/your-project/schemas/events-schema" },
    { id = "processed_schema", service_name = "pubsub-example", resource = "projects/your-project/schemas/processed-events" }
]
# Middleware with different encoding transformations
middlewares = [
    { service_name = "transform-service", resource = "process", schema_id = "events_schema", output_encoding = "json" },
    # Result is sent as JSON to the next middleware
    { service_name = "http-local", resource = "analyze", schema_id = "processed_schema", output_encoding = "json" },
]
sinks = [
    { service_name = "pubsub-example", resource = "projects/your-project/topics/processed-events", output_encoding = "avro", schema_id = "processed_schema" },
]

# examples for batch processing

[[connectors]]
name = "batch-mongodb-connector"
# Configure batch processing with a count of 100 events per batch
batch = { kind = "count", size = 100 }
source = { service_name = "mongodb-source", resource = "high_volume_collection", output_encoding = "bson" }
sinks = [
    { service_name = "kafka-local", resource = "batched_events", output_encoding = "json" }
]

[[connectors]]
name = "batch-with-schema-connector"
# Larger batch size for high-throughput processing
batch = { kind = "count", size = 500 }
source = { service_name = "mongodb-source", resource = "metrics_collection", output_encoding = "bson", schema_id = "metrics_schema" }
schemas = [
    { id = "metrics_schema", service_name = "pubsub-example", resource = "projects/your-project/schemas/metrics-schema" }
]
sinks = [
    { service_name = "kafka-cloud", resource = "metrics_topic", output_encoding = "avro", schema_id = "metrics_schema" }
]

[[connectors]]
name = "batch-middleware-connector"
# Batch processing with middleware transformations
batch = { kind = "count", size = 200 }
source = { service_name = "kafka-cloud", resource = "batch_events", input_encoding = "avro", output_encoding = "json", schema_id = "batch_schema" }
schemas = [
    { id = "batch_schema", service_name = "pubsub-example", resource = "projects/your-project/schemas/batch-events-schema" }
]
middlewares = [
    # Process batches of events at once for better performance
    { service_name = "transform-service", resource = "batch_process", schema_id = "batch_schema", output_encoding = "json" }
]
sinks = [
    { service_name = "pubsub-example", resource = "projects/your-project/topics/processed-batches", output_encoding = "avro", schema_id = "batch_schema" }
]
